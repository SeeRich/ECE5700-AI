{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Review of Linear Algebra\n",
    "\n",
    "Content and structure mainly from: http://www.deeplearningbook.org/contents/linear_algebra.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalars\n",
    "\n",
    "- Single number\n",
    "- Denoted as lowercase letter\n",
    "- Examples\n",
    "    - $x \\in \\mathbb{R}$ - Real number\n",
    "    - $y \\in \\{0, 1, \\dots, C\\}$ - Finite set\n",
    "    - $u \\in [0, 1]$ - Bounded set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = 1.1343\n",
    "print(x)\n",
    "z = int(-5)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors\n",
    "\n",
    "- Array of numbers\n",
    "- In notation, we usually consider vectors to be \"column vectors\"\n",
    "- Denoted as lowercase letter (often bolded)\n",
    "- Dimension is often denoted by $d$, $D$, or $p$.\n",
    "- Access elements via subscript, e.g., $x_i$ is the $i$-th element\n",
    "- Examples\n",
    "    - $\\mathbf{x} \\in \\mathbb{R}^d$\n",
    "    - $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} $\n",
    "    - $\\mathbf{x} = [x_1, x_2, \\dots, x_d]^T$\n",
    "    - $\\mathbf{z} = [\\sqrt{x_1}, \\sqrt{x_2}, \\dots, \\sqrt{x_d}]^T$\n",
    "    - $\\mathbf{y} \\in \\{0, 1, \\dots, C\\}^d$ - Finite set\n",
    "    - $\\mathbf{u} \\in [0, 1]^d$ - Bounded set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.1343, 6.2345, 35])\n",
    "print(x)\n",
    "z = 5 * np.ones(3, dtype=int)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: The operator `+` does different things on numpy arrays vs Python lists\n",
    "\n",
    "- For lists, Python concatenates the lists\n",
    "- For numpy arrays, numpy performs an element-wise addition\n",
    "- Similarly, for other binary operators such as `-`, `+`, `*`, and `/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = [1, 2]\n",
    "b_list = [30, 40]\n",
    "c_list = a_list + b_list\n",
    "print(c_list)\n",
    "a = np.array(a_list)  # Create numpy array from Python list\n",
    "b = np.array(b_list)\n",
    "c = a + b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices\n",
    "\n",
    "- 2D array of numbers\n",
    "- Denoted as uppercase letter\n",
    "- Number of samples often denoted by $n$ or $N$.\n",
    "- Access rows or columns via subscript or numpy notation:\n",
    "    - $X_{i,:}$ is the $i$-th row, $X_{:,j}$ is the $j$th column\n",
    "    - (Sometimes) $X_i$, $\\mathbf{x}_i$ is the $i$-th row or column depending on context\n",
    "- Access elements by double subscript $X_{i,j}$ or $x_{i,j}$ is the $i,j$-th entry of the matrix\n",
    "- Examples\n",
    "    - $X \\in \\mathbb{R}^{n \\times d}$ - Real number\n",
    "    - $X = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$ - Real number\n",
    "    - $Y \\in \\{0, 1, \\dots, C\\}^{k \\times d}$ - Finite set\n",
    "    - $U \\in [0, 1]^{n \\times d}$ - Bounded set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(12).reshape(3,4)\n",
    "print(X)\n",
    "W = np.array([\n",
    "    [1.1343 + 2.1j, 1j, 0.1 + 3.5j],\n",
    "    [3, 4, 5],\n",
    "])\n",
    "print(W)\n",
    "Z = 5 * np.ones((3, 3), dtype=int)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "- $n$-D arrays\n",
    "- Examples\n",
    "    - $X \\in \\mathbb{R}^{3 \\times h \\times w}$, single color image in PyTorch\n",
    "    - $X \\in \\mathbb{R}^{n \\times 3 \\times h \\times w}$, multiple color images in PyTorch\n",
    "    - $X \\in \\mathbb{R}^{h \\times w \\times 3}$, single color image for matplotlib imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "china = load_sample_image('china.jpg')\n",
    "print('Shape of image (height, width, channels):', china.shape)\n",
    "ax = plt.axes(xticks=[], yticks=[])\n",
    "ax.imshow(china);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix transpose\n",
    "\n",
    "- Changes columns to rows and rows to columns \n",
    "- Denoted as $A^T$\n",
    "- For vectors $\\mathbf{v}$, the transpose changes from a column vector to a row vector\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\mathbf{x} &= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}, &\n",
    "    \\mathbf{x}^T &= \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix}^T = [x_1, x_2, \\dots, x_d]\n",
    "    \\end{align}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(6).reshape(2,3)\n",
    "print(A)\n",
    "print(A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the transpose of a row vector (i.e., 1D array) in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.arange(5)\n",
    "print(v)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What will be the output of the following? ---- Discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v.T)\n",
    "print(v.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for discussion question\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "###  NOTE: In numpy, there is only a \"vector\" (i.e., a 1D array), not really a row or column vector per se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v = np.arange(5)\n",
    "print('A numpy vector', v)\n",
    "print('Transpose of numpy vector', v.T)\n",
    "print('A matrix with one column')\n",
    "print(v.shape)\n",
    "print(len(v.shape))\n",
    "V = v.reshape(-1, 1)\n",
    "print('V shape: ', V.shape)\n",
    "print(V)\n",
    "np.dot(v.T, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix product\n",
    "\n",
    "- Let $A \\in \\mathbb{R}^{m \\times n}$, $B \\in \\mathbb{R}^{n \\times p}$, then the **matrix product** $ C = AB $ is defined as:\n",
    "  $$ c_{i,j} = \\sum_{k \\in \\{1,2,\\dots,n\\}} a_{i,k} b_{k,j}$$\n",
    "  where $C \\in \\mathbb{R}^{m \\times p}$ (notice how inner dimension is collapsed.\n",
    "- (Show on board visually)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(6).reshape(3, 2)\n",
    "print(A)\n",
    "B = np.arange(6).reshape(2, 3)\n",
    "print(B)\n",
    "C = np.zeros((A.shape[0], B.shape[1]))\n",
    "print(C.shape)\n",
    "for i in range(C.shape[0]):\n",
    "    for j in range(C.shape[1]):\n",
    "        for k in range(A.shape[1]):\n",
    "            C[i, j] += A[i, k] * B[k, j]\n",
    "print(C)\n",
    "print(np.matmul(A, B))\n",
    "print(A @ B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice triple loop, naively cubic complexity $O(n^3)$\n",
    "\n",
    "### However, special linear algebra algorithms can do it $O(n^{2.803})$\n",
    "### Takeaway - Use numpy `np.matmul` or `@` operator for matrix multiplication \n",
    "(`np.dot` also works for matrix multiplication but is different in PyTorch and is less explicit so I suggest the two methods above for matrix multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Element-wise (Hadamard) product *NOT equal* to matrix multiplication\n",
    "\n",
    "- Normal matrix mutiplication $C = AB$ is very different from **element-wise** (or more formally **Hadamard**) multiplication, denoted $F = A \\odot D$, which in numpy is just the star `*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(6).reshape(3, 2)\n",
    "print(A)\n",
    "B = np.arange(6).reshape(2, 3)\n",
    "print(B)\n",
    "try:\n",
    "    A * B  # Fails since matrix shapes don't match and cannot broadcast\n",
    "except ValueError as e:\n",
    "    print('Operation failed! Message below:')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)\n",
    "D = 10*B.T\n",
    "print(D)\n",
    "F = A * D  # Element-wise / Hadamard product\n",
    "print(F)\n",
    "\n",
    "print(2*F)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of matrix product\n",
    "\n",
    "- Distributive: $A(B+C) = AB + AC$\n",
    "- Associative: $A(BC) = (AB)C$\n",
    "- **NOT** commutative, i.e., $AB = BA$ does **NOT** always hold\n",
    "- Transpose of multiplication (**switch order** and transpose of both):\n",
    "    $$(AB)^T = B^T A^T$$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AB')\n",
    "print(np.matmul(A, B))\n",
    "print('BA')\n",
    "print(np.matmul(B, A))\n",
    "print('(AB)^T')\n",
    "print((A @ B).T)\n",
    "print('B^T A^T')\n",
    "print(np.dot(B.T, A.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of inner product or vector-vector product\n",
    "\n",
    "- **Inner product** or **vector-vector** multiplication produces *scalar*:\n",
    "  $$\\mathbf{x}^T \\mathbf{y} = (\\mathbf{x}^T \\mathbf{y})^T = \\mathbf{y}^T \\mathbf{x}$$\n",
    "  Also denoted as:\n",
    "  $$\\langle \\mathbf{x}, \\mathbf{y}\\rangle = \\mathbf{x}^T \\mathbf{y}$$\n",
    "  Can be executed via `np.dot` or `np.matmul`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner product\n",
    "a = np.arange(3)\n",
    "print(a)\n",
    "b = np.array([11, 22, 33])\n",
    "print(b)\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity matrix keeps vectors unchanged \n",
    "\n",
    "- Multiplying by the identity does not change vector (generalizing the concept of the scalar 1)\n",
    "- Formally, $I_n \\in \\mathbb{R}^{n \\times n}$, and $\\forall \\mathbf{x} \\in \\mathbb{R}^n, I_n \\mathbf{x} = \\mathbf{x}$\n",
    "- Structure is ones on the diagonal, zero everywhere else:\n",
    "- `np.eye` function to create identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I3 = np.eye(3)\n",
    "print(I3)\n",
    "x = np.random.randn(3)\n",
    "print(x)\n",
    "print(np.matmul(I3, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix inverse times the original matrix is the identity\n",
    "\n",
    "- The inverse of *square* matrix $A \\in \\mathbb{n \\times n}$ is denoted as $A^{-1}$ and defined as:\n",
    "    $$ A^{-1} A = I $$\n",
    "- The \"right\" inverse is similar and is equal to the left inverse:\n",
    "    $$ A A^{-1} = I $$\n",
    "- Generalizes the concept of inverse $x$ and $\\frac{1}{x}$\n",
    "- Does **NOT** always exist, similar to how the inverse of $x$ only exists if $x \\neq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A = 100 * np.array([[1, 0.5], [0.2, 1]])\n",
    "print(A)\n",
    "Ainv = np.linalg.inv(A)\n",
    "print(Ainv)\n",
    "print('A^{-1} A = ')\n",
    "print(np.matmul(Ainv, A))\n",
    "print('A A^{-1} = ')\n",
    "print(np.matmul(A, Ainv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular matrices are similar to zeros\n",
    "\n",
    "- Informally, singular matrices are matrices that do not have an inverse (similar to the idea that 0 does not have an inverse)\n",
    "- Consider the 1D equation $ax = b$\n",
    "    - Usually we can solve for $x$ by multiplying both sides by $1/a$\n",
    "    - But what if $a = 0$?\n",
    "    - What are the solutions to the equation?\n",
    "- Called \"singular\" because a random matrix is unlikely to be singular just like choosing a random number is unlikely to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import LinAlgError\n",
    "def try_inv(A):\n",
    "    print('A = ')\n",
    "    print(np.array(A))\n",
    "    try: \n",
    "        np.linalg.inv(A)\n",
    "    except LinAlgError as e:\n",
    "        print(e)\n",
    "    else:\n",
    "        print('Not singular!')\n",
    "    print()\n",
    "        \n",
    "#try_inv([[0, 0], [0, 0]])\n",
    "#try_inv(np.eye(3))\n",
    "#try_inv([[1, 1], [1, 1]])\n",
    "#try_inv([[1, 10], [1, 10]])\n",
    "#try_inv([[2, 20], [4, 40]])\n",
    "try_inv([[2, 20], [40, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random matrix is very unlikely to be 0\n",
    "for j in range(10):\n",
    "    try_inv(np.random.randn(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norms: The \"size\" of a vector or matrix\n",
    "\n",
    "- Informally, a generalization of the absolute value of a scalar\n",
    "- Formally, a norm is an function $f$ that has the following three properties:\n",
    "    - $f(\\mathbf{x}) = 0 \\Rightarrow \\mathbf{x} = \\mathbf{0}$ (zero point)\n",
    "    - $f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y})$ (Triangle inequality)\n",
    "    - $\\forall \\alpha \\in \\mathbb{R}, f(\\alpha \\mathbf{x}) = |\\alpha|f(\\mathbf{x})$ (absolutely homogenous) \n",
    "- Examples\n",
    "    - Absolute value of scalars\n",
    "    - $p$-norm (also denoted $\\ell_p$-norm)\n",
    "    $$ \\| \\mathbf{x} \\|_p = \\left(\\sum_{i=1}^d |x_i|^p\\right)^{\\frac{1}{p}} $$\n",
    "    - (Discussion) What does this represent when $p=2$ (for simplicity you can assume $d=2$)?\n",
    "        - When $p=2$, we often merely denote as $\\|\\mathbf{x}\\|$.\n",
    "    - What about when $p=1$?\n",
    "    - What about when $p=\\infty$ (or more formally the limit as $p \\to \\infty$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 1])\n",
    "print(np.linalg.norm(x, ord=2))\n",
    "print(np.linalg.norm(x, ord=1))\n",
    "print(np.linalg.norm(x, ord=np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors that have the same norm form a \"ball\" that isn't necessarily circular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(1000, 2)\n",
    "\n",
    "p_vals = [1, 1.5, 2, 4, np.inf]\n",
    "fig, axes = plt.subplots(1, len(p_vals), figsize=(len(p_vals)*4, 3))\n",
    "\n",
    "for p, ax in zip(p_vals, axes):\n",
    "    # Normalize them to have the unit norm\n",
    "    Z = (X.T / np.linalg.norm(X, ord=p, axis=1)).T\n",
    "    ax.scatter(Z[:, 0], Z[:, 1])\n",
    "    ax.axis('equal')\n",
    "    ax.set_title('Unit Norm Ball for $p$=%g' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Squared* $L_2$ norm is quite common since it simplifies to a simple summation\n",
    "\n",
    "$$ \\|\\mathbf{x}\\|_2^2 = \\left(\\left(\\sum_{i=1}^d |x_i|^2 \\right)^{\\frac{1}{2}}\\right)^2 = \\sum_{i=1}^d |x_i|^2 = \\sum_{i=1}^d x_i^2 $$\n",
    "\n",
    "- Additionally, this can be computed as $\\|\\mathbf{x}\\|_2^2 = \\mathbf{x}^T \\mathbf{x}$\n",
    "- Informally, this is analogous to taking the square of a scalar number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(4)\n",
    "print(np.linalg.norm(x, ord=2)**2)\n",
    "print(np.dot(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal vectors\n",
    "\n",
    "- Orthogonal vectors are vectors such that $\\mathbf{x}^T \\mathbf{y} = 0$\n",
    "- The dot product between vectors can be written in terms of norms and the cosine of the angle:\n",
    "    $$ \\mathbf{x}^T \\mathbf{y} = \\|\\mathbf{x}\\|_2 \\|\\mathbf{y}\\|_2 \\cos \\theta $$\n",
    "- (Discussion) Suppose $\\mathbf{x}$ and $\\mathbf{y}$ are non-zero vectors, what must $\\theta$ be if the vectors are orthogonal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.matmul([0, 1], [1, 0]))\n",
    "theta = np.pi/2\n",
    "x = np.array([np.cos(theta), -np.sin(theta)])\n",
    "y = np.array([np.sin(theta), np.cos(theta)])\n",
    "print(x)\n",
    "print(y)\n",
    "print(np.dot(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special matrices: Orthogonal matrices\n",
    "\n",
    "- Informally, an orthogonal matrix only rotates (or reflects) vectors around the origin (zero point), but does not change the size of the vectors.\n",
    "- Informally, almost analagous to a 1 or -1 for matrices but more general\n",
    "- A *square* matrix such that $Q^T Q = Q Q^T = I$\n",
    "- Or, equivalently $Q^{-1} = Q^T$\n",
    "- Or, equivalently:\n",
    "    - Every column (or row) is orthogonal to every other column (or row)\n",
    "    - Every column (or row) has unit $\\ell_2$-norm, i.e., $\\|Q_{i,:}\\|_2 = \\|Q_{:,j}\\|_2 = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Identity matrix')\n",
    "Q = np.eye(2) # Identity\n",
    "print(Q)\n",
    "print(np.allclose(np.eye(2), np.matmul(Q.T, Q)))\n",
    "\n",
    "print('Reflection matrix')\n",
    "Q = np.array([[1, 0], [0, -1]]) # Reflection\n",
    "print(Q)\n",
    "print(np.allclose(np.eye(2), np.matmul(Q.T, Q)))\n",
    "\n",
    "print('Rotation matrix')\n",
    "theta = np.pi/3\n",
    "Q = np.array([\n",
    "    [np.cos(theta), -np.sin(theta)],\n",
    "    [np.sin(theta), np.cos(theta)]\n",
    "])\n",
    "print(Q)\n",
    "print(np.allclose(np.eye(2), np.matmul(Q.T, Q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other special matrices: Symmetric, Triangular, Diagonal\n",
    "\n",
    "- Symmetric matrices are symmetric around the diagonal; formally, $A = A^T$\n",
    "- Triangular matrices only have non-zeros in the upper or lower triangular part of the matrix\n",
    "- Diagonal matrices only have non-zeros along the diagonal of a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(25).reshape(5, 5)+1\n",
    "print('Symmetric')\n",
    "print(A + A.T)\n",
    "print('Upper triangular')\n",
    "print(np.triu(A))\n",
    "print('Lower triangular')\n",
    "print(np.tril(A))\n",
    "print('Diagonal (both upper and lower triangular)')\n",
    "print(np.diag(np.arange(5) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplying a matrix by a diagonal matrix scales the columns or rows\n",
    "\n",
    "- Right multiplication scales columns\n",
    "- Left multiplication scales rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(16).reshape(4, 4)\n",
    "print(A)\n",
    "D = np.diag(10**(np.arange(4)))\n",
    "diag_vec = np.diag(D)\n",
    "print(D)\n",
    "print('AD')\n",
    "print(np.matmul(A, D))\n",
    "print('AD (via numpy * and broadcasting)')\n",
    "print(A * diag_vec)\n",
    "print('DA')\n",
    "print(np.matmul(D, A))\n",
    "print('DA (via numpy * and broadcasting)')\n",
    "print(A * diag_vec.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse of diagonal matrix is formed merely by taking inverse of diagonal elements\n",
    "\n",
    "- Most operations on diagonal matrices are just the scalar versions of their entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.diag(np.arange(5)+1)\n",
    "print(A)\n",
    "diag_A = np.diag(A)\n",
    "print('diag_A', diag_A)\n",
    "diag_A_inv = 1 / diag_A\n",
    "print('diag_A_inv', diag_A_inv)\n",
    "Ainv = np.diag(diag_A_inv)\n",
    "print(Ainv)\n",
    "Ainv_full = np.linalg.inv(A)\n",
    "print(Ainv_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation: Matrix decompositions allow us to *understand* and *manipulate* matrices both theoretically and practically\n",
    "\n",
    "- Analagous to prime factorization of an integer, e.g., $12 = 2 \\times 2 \\times 3$\n",
    "    - Allows us to determine whether things are divisible by other integers\n",
    "- Analagous to representing a signal in the time versus frequency domain\n",
    "    - Both domains represent the same object but are useful for different computations and derivations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigendecomposition\n",
    "\n",
    "- For real **symmetric** matrices, the eigendecomposition is:\n",
    "    $$ A = Q \\Lambda Q^T $$\n",
    "  where $Q$ is an **orthogonal** matrix and $\\Lambda$ is a **diagonal** matrix.\n",
    "- Often *in notation*, it is assumed that the diagonal of $\\Lambda$, denoted $\\mathbf{\\lambda}$ is ordered by decreasing values, i.e., $\\lambda_1 \\geq \\lambda_2, \\geq \\cdots \\geq \\lambda_d$. \n",
    "- $\\mathbf{\\lambda}$ are known as the **eigenvalues** and $Q$ is known as the **eigenvector matrix**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "B = rng.randn(4,4)\n",
    "A = B + B.T # Make symmetric\n",
    "lam, Q = np.linalg.eig(A)\n",
    "print(np.diag(lam))\n",
    "print(Q)\n",
    "A_reconstructed = np.matmul(np.matmul(Q, np.diag(lam)), Q.T)\n",
    "print('Are all entries equal up to machine precision?')\n",
    "print('Yes' if np.allclose(A, A_reconstructed) else 'No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple properties based on eigendecomposition\n",
    "\n",
    "- $A^{-1}$ is easy to compute \n",
    "    - Easy to solve equation $A\\mathbf{x} = \\mathbf{b}$\n",
    "- Powers of matrix is easy to compute $A^3 = A A A$.\n",
    "- The matrix is singular if and only if there is a zero in $\\mathbf{\\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular value decomposition of *any* matrix (The decomposition to end all decompositions)\n",
    "\n",
    "- For **any** matrix $A \\in \\mathbb{R}^{m \\times n}$ (even non-square), the singular value decomposition is:\n",
    "    $$ A = U \\Sigma V^T $$\n",
    "  where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are **orthogonal** matrices and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a **diagonal** (though not necessarily square) matrix.\n",
    "- Often in notation, it is assumed that the diagonal of $\\Sigma$, denoted $\\mathbf{\\sigma}$ is ordered by decreasing values, i.e., $\\sigma_1 \\geq \\sigma_2, \\geq \\cdots \\geq \\sigma_d$. \n",
    "- $\\mathbf{\\sigma}$ are known as the **singular values** and $U$ and $V$ are known as the **left singular vectors** and the **right singular vectors** respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "A = np.arange(6).reshape(2, 3)\n",
    "print('A', A.shape)\n",
    "print(A)\n",
    "\n",
    "# Note returns V^T (i.e. transpose) rather than V\n",
    "U, s, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "# Convert singular vector to matrix\n",
    "Sigma = np.zeros_like(A, dtype=float)\n",
    "Sigma[:2, :2] = np.diag(s)\n",
    "\n",
    "print('U', U.shape)\n",
    "print('Sigma', Sigma.shape)\n",
    "print('Vt', Vt.shape)\n",
    "\n",
    "A_reconstructed = np.matmul(U, np.matmul(Sigma, Vt))\n",
    "print('Are all entries equal up to machine precision?')\n",
    "print('Yes' if np.allclose(A, A_reconstructed) else 'No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Rank* $\\text{rank}(A)$ is the number of linearly independent columns\n",
    "\n",
    "- Consider an example of two equations with two unknowns (Is there a unique solution?):\n",
    "    - $2x + 3y = 0$\n",
    "    - $4x + 6y = 1$\n",
    "- Similar to a matrix $A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 6 \\end{bmatrix}$, notice \"redundancy\"\n",
    "- SVD -> Rank = Number of non-zero singular values\n",
    "- If $A \\in \\mathbb{R}^{d \\times d}$, $A$ is not singular if and only if $\\text{rank}(A) = d$.\n",
    "- Simplest case is rank 1 matrix: $\\mathbf{x}\\mathbf{y}^T$ (show on board)\n",
    "    - **Notice difference from inner product, denoted as $\\mathbf{x}^T\\mathbf{y}$**\n",
    "    - $\\mathbf{x} \\mathbf{y}^T$ is also known as the **outer product** of two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD provides powerful interpretation of matrix as sum of rank one matrices\n",
    "\n",
    "$$ A = U \\Sigma V^T = \\sum_{i=1}^{\\text{rank}(A)} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T $$\n",
    "\n",
    "- SVD can be used to solve the following matrix approximation problem:\n",
    "    $$ \\min_B \\|A - B\\|_F \\quad \\text{s.t.} \\quad \\text{rank}(B) \\leq r $$\n",
    "    where $\\|A\\|_F$ is the Frobenius norm, or just like the $\\ell_2$-norm but consider the matrix as a long vector.\n",
    "    - Example: $$\\|A\\|_F = \\left\\|\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\right\\|_F = \\|[a,b,c,d]\\|_2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "china = load_sample_image('china.jpg')\n",
    "gray_china = china[:,:,0]/255.0\n",
    "print('china matrix', gray_china.shape)\n",
    "#print(gray_china)\n",
    "\n",
    "U, s, Vt = np.linalg.svd(gray_china)\n",
    "Sigma = np.zeros_like(gray_china, dtype=float)\n",
    "Sigma[:427, :427] = np.diag(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = np.min(gray_china.shape)\n",
    "rank_arr = [0,1, 2, 4, 8, 16, 32, 64,  max_rank]\n",
    "fig, axes = plt.subplots(3, 3, figsize=(len(rank_arr)*2, 3*4))\n",
    "for r, ax in zip(rank_arr, axes.ravel()):\n",
    "    china_approx = np.matmul(U[:, :r], np.matmul(Sigma[:r,:r], Vt[:r, :]))\n",
    "    compression = r/max_rank\n",
    "    ax.imshow(china_approx, cmap='gray')\n",
    "    ax.set_title('Rank=%d, Compression=%.1f%%' % (r, compression*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the difference between the approximation and the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = np.min(gray_china.shape)\n",
    "rank_arr = [0,1, 2, 4, 8, 16, 32, 64, max_rank]\n",
    "fig, axes = plt.subplots(3, 3, figsize=(len(rank_arr)*2, 3*4))\n",
    "for r, ax in zip(rank_arr, axes.ravel()):\n",
    "    china_diff = np.matmul(U[:, :r], np.matmul(Sigma[:r,:r], Vt[:r, :])) - gray_china\n",
    "    compression = r/max_rank\n",
    "    ax.imshow(china_diff, cmap='gray', vmin=-1, vmax=1)\n",
    "    ax.set_title('Difference, Rank=%d, Compression=%.1f%%' % (r, compression*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usually the most important information is in the first few singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most important components are \n",
    "plt.plot(s,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Trace* $\\text{Tr}(A)$ operation\n",
    "\n",
    "- Trace is just the sum of the diagonal elements of a matrix\n",
    "    $$ \\text{Tr}(A) = \\sum_{i=1}^d a_{i,i} $$\n",
    "- Most useful property is rotational equivalence:\n",
    "    $$ \\text{Tr}(ABC) = \\text{Tr}(CAB) = \\text{Tr}(BCA) $$\n",
    "- In particular, (even if different dimensions)\n",
    "    $$ \\text{Tr}(AB) = \\text{Tr}(BA) $$\n",
    "- Also, trace operator is linear so we have the following properties:\n",
    "    $$ \\text{Tr}(\\alpha A + \\beta B) = \\alpha\\text{Tr}(A) + \\beta\\text{Tr}(B) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(2*3).reshape(2,3)\n",
    "B = A.copy().T\n",
    "print('AB')\n",
    "print(np.matmul(A, B))\n",
    "print('Tr(AB)')\n",
    "print(np.trace(np.matmul(A, B)))\n",
    "print('Tr(BA)')\n",
    "print(np.trace(np.matmul(B, A)))\n",
    "print('Tr(A^T B^T)')\n",
    "print(np.trace(np.matmul(A.T, B.T)))\n",
    "print('Tr(B^T A^T)')\n",
    "print(np.trace(np.matmul(B.T, A.T)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
